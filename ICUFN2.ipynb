{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "wandb.init(project='Federated Learning', entity='yhkim')\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "# mnist 학습, 성능 검증 데이터셋 나누기\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)\n",
    "x_test = x_test.reshape(10000, 28, 28, 1)\n",
    "x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
    "x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
    "idx = np.argsort(y_test)\n",
    "x_train_sorted = x_test[idx]\n",
    "y_train_sorted = y_test[idx]\n",
    "x_eval_dataset = x_train[29854:]\n",
    "y_eval_dataset = y_train[29854:]\n",
    "\n",
    "# 균형 데이터셋 5세트\n",
    "UE = []\n",
    "for _ in range(5):\n",
    "    UE.append({\"x_train\": [], \"y_train\": []})\n",
    "\n",
    "random.seed(45)\n",
    "\n",
    "total = 0\n",
    "random_num_list = []\n",
    "for _ in range(5):\n",
    "    random_num = random.randrange(1000, 12000)\n",
    "    total += random_num\n",
    "    random_num_list.append(random_num)\n",
    "\n",
    "print('사용한 데이터 수 :', total)\n",
    "print(random_num_list)\n",
    "\n",
    "start = 0\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        UE[i]['x_train'] = x_train[:random_num_list[i]]\n",
    "        UE[i]['y_train'] = y_train[:random_num_list[i]]\n",
    "    else:\n",
    "        UE[i]['x_train'] = x_train[start:start+random_num_list[i]]\n",
    "        UE[i]['y_train'] = y_train[start:start+random_num_list[i]]\n",
    "    start += random_num_list[i]\n",
    "\n",
    "# 불균형 데이터 5개 세트\n",
    "UE.append({'x_train': x_test[y_test == 0],\n",
    "           'y_train': y_test[y_test == 0]})  # UE6\n",
    "\n",
    "x = x_test[y_test == 1]\n",
    "y = y_test[y_test == 1]\n",
    "x = np.append(x, x_test[y_test == 3], axis=0)\n",
    "y = np.append(y, y_test[y_test == 3], axis=0)\n",
    "\n",
    "UE.append({'x_train': x, 'y_train': y})  # UE7\n",
    "\n",
    "x = x_test[y_test == 2]\n",
    "y = y_test[y_test == 2]\n",
    "x = np.append(x, x_test[y_test == 4], axis=0)\n",
    "y = np.append(y, y_test[y_test == 4], axis=0)\n",
    "x = np.append(x, x_test[y_test == 9], axis=0)\n",
    "y = np.append(y, y_test[y_test == 9], axis=0)\n",
    "\n",
    "UE.append({'x_train': x, 'y_train': y})  # UE8\n",
    "\n",
    "x = x_test[y_test == 5]\n",
    "y = y_test[y_test == 5]\n",
    "x = np.append(x, x_test[y_test == 6], axis=0)\n",
    "y = np.append(y, y_test[y_test == 6], axis=0)\n",
    "\n",
    "UE.append({'x_train': x, 'y_train': y})  # UE9\n",
    "\n",
    "x = x_test[y_test == 8]\n",
    "y = y_test[y_test == 8]\n",
    "x = np.append(x, x_test[y_test == 7], axis=0)\n",
    "y = np.append(y, y_test[y_test == 7], axis=0)\n",
    "\n",
    "UE.append({'x_train': x, 'y_train': y})  # UE10\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = [], [], [], []\n",
    "for i in range(10):\n",
    "    x_train_temp, x_test_temp, y_train_temp, y_test_temp = train_test_split(\n",
    "        UE[i]['x_train'], UE[i]['y_train'], test_size=0.2, random_state=45)\n",
    "    x_train.append(x_train_temp)\n",
    "    x_test.append(x_test_temp)\n",
    "    y_train.append(y_train_temp)\n",
    "    y_test.append(y_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_accuracy = []  # 10개의 UE가 각각 학습한 가중치를 평균내어 산출한 정확도\n",
    "global_loss = []  # 10개의 UE가 각각 학습한 가중치를 평균내어 산출한 loss\n",
    "for round in range(100):  # Communication Round, Global epoch\n",
    "    # 각 UE 학습\n",
    "    learning_result_list = []\n",
    "    for i in range(10):\n",
    "        if os.path.isdir('fl_model'):\n",
    "            model = tf.keras.models.load_model('fl_model')\n",
    "            model.compile(optimizer='SGD',\n",
    "                          loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            learning_result_list.append(model.fit(\n",
    "                x_train[i], y_train[i], batch_size=100, epochs=1, validation_data=(x_test[i], y_test[i])))\n",
    "            tf.keras.backend.clear_session()\n",
    "        else:\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.Conv2D(filters=6, kernel_size=(5, 5),\n",
    "                                    strides=1, activation='tanh', input_shape=(32, 32, 1)))\n",
    "            model.add(layers.AveragePooling2D(pool_size=2, strides=2))\n",
    "            model.add(layers.Conv2D(filters=16, kernel_size=(\n",
    "                5, 5), strides=1, activation='tanh'))\n",
    "            model.add(layers.AveragePooling2D(pool_size=2, strides=2))\n",
    "            model.add(layers.Flatten())\n",
    "            model.add(layers.Dense(120, activation='tanh'))\n",
    "            model.add(layers.Dense(84, activation='tanh'))\n",
    "            model.add(layers.Dense(10, activation='softmax'))\n",
    "            model.compile(optimizer='SGD',\n",
    "                          loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # lr = 0.01\n",
    "            learning_result_list.append(model.fit(\n",
    "                x_train[i], y_train[i], batch_size=100, epochs=1, validation_data=(x_test[i], y_test[i])))\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    # 각 UE 학습 리스트 안에 딕셔너리로 저장\n",
    "    UE_weights = []\n",
    "\n",
    "    for model in learning_result_list:\n",
    "        layer_weights = {}\n",
    "        for x in model.model.layers:\n",
    "            if len(x.get_weights()) > 0:\n",
    "                layer_weights[x.name] = x.get_weights()\n",
    "        UE_weights.append(layer_weights)\n",
    "\n",
    "    # 서버 모델 생성\n",
    "    server_model = models.Sequential()\n",
    "    server_model.add(layers.Conv2D(filters=6, kernel_size=(\n",
    "        5, 5), strides=1, activation='tanh', input_shape=(32, 32, 1)))\n",
    "    server_model.add(layers.AveragePooling2D(pool_size=2, strides=2))\n",
    "    server_model.add(layers.Conv2D(\n",
    "        filters=16, kernel_size=(5, 5), strides=1, activation='tanh'))\n",
    "    server_model.add(layers.AveragePooling2D(pool_size=2, strides=2))\n",
    "    server_model.add(layers.Flatten())\n",
    "    server_model.add(layers.Dense(120, activation='tanh'))\n",
    "    server_model.add(layers.Dense(84, activation='tanh'))\n",
    "    server_model.add(layers.Dense(10, activation='softmax'))\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # 10개 UE가 각자 학습한 가중치 취합\n",
    "    sum_weights = {}\n",
    "\n",
    "    for i in range(len(list(UE_weights[0].keys()))):\n",
    "        weight_shape = [0]\n",
    "        bias_shape = [0]\n",
    "        for dim in UE_weights[0][list(UE_weights[0].keys())[i]][0].shape:\n",
    "            weight_shape.append(dim)\n",
    "        for dim in UE_weights[0][list(UE_weights[0].keys())[i]][1].shape:\n",
    "            bias_shape.append(dim)\n",
    "        sum_weights.update({list(UE_weights[0].keys())[i]: {\n",
    "            'weight': np.empty(weight_shape), 'bias': np.empty(bias_shape)}})\n",
    "\n",
    "        for UE in UE_weights:\n",
    "            sum_weights[list(UE.keys())[i]]['weight'] = np.append(\n",
    "                sum_weights[list(UE.keys())[i]]['weight'], [UE[list(UE.keys())[i]][0]], axis=0)\n",
    "            sum_weights[list(UE.keys())[i]]['bias'] = np.append(\n",
    "                sum_weights[list(UE.keys())[i]]['bias'], [UE[list(UE.keys())[i]][1]], axis=0)\n",
    "\n",
    "    # 서버 모델에 가중치 설정하는 코드\n",
    "    for layer in sum_weights.keys():\n",
    "        for model_layer in server_model.layers:\n",
    "            if layer == model_layer.name:\n",
    "                model_layer.set_weights([np.mean(sum_weights[layer]['weight'], axis=0), np.mean(  # 각 UE 가중치 평균값으로 FL\n",
    "                    sum_weights[layer]['bias'], axis=0)])\n",
    "\n",
    "    # FL 성능 검증\n",
    "    server_model.compile(\n",
    "        optimizer='SGD', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    result = server_model.evaluate(\n",
    "        x=x_eval_dataset, y=y_eval_dataset, batch_size=100)\n",
    "    print('test loss, test acc:', result)\n",
    "    server_model.save('fl_model')\n",
    "    global_loss.append(result[0])\n",
    "    global_accuracy.append(result[1])\n",
    "    wandb.log(\n",
    "        {'global accuracy': result[1], 'global loss': result[0], 'global epoch': round+1})\n",
    "\n",
    "with open('global_accuracy_result.txt', 'w') as f:\n",
    "    for x in global_accuracy:\n",
    "        f.write(x+'\\n')\n",
    "\n",
    "with open('global_loss_result.txt', 'w') as f:\n",
    "    for x in global_loss:\n",
    "        f.write(x+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-lloyd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
