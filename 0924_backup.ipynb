{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unusual-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "def gen_ue_cluster(UE_sort):\n",
    "    UE_cluster = []\n",
    "    UE_group = []\n",
    "    UE_sort = OrderedDict({'2': 0, '9': 1, '0': 2, '3': 2, '4': 3, '6': 5, '7': 5, '8': 5, '5': 5, '1': 5})\n",
    "    UE_distance = [] # 2 UE 간의 거리 리스트\n",
    "    UE_len = len(UE_sort)\n",
    "    avg_distance = 0\n",
    "    UE_sort_keys = list(UE_sort.keys())\n",
    "    for i in range(0, UE_len - 1):\n",
    "        x1, y1 = i + 1, UE_sort[UE_sort_keys[i]]\n",
    "        x2, y2 = i+2, UE_sort[UE_sort_keys[i+1]]\n",
    "        distance = (((x2 - x1) ** 2) + ((y2 - y1) ** 2)) ** 0.5\n",
    "        avg_distance += distance\n",
    "        UE_distance.append((UE_sort_keys[i], UE_sort_keys[i+1],distance))\n",
    "    avg_distance = avg_distance/len(UE_sort)\n",
    "    # print(f'avg distance : {avg_distance/len(UE_sort)}')\n",
    "    for i, x in enumerate(UE_distance):\n",
    "        if x[2] > avg_distance:\n",
    "            UE_group.append(x[0])\n",
    "            UE_cluster.append(UE_group.copy())\n",
    "            UE_group.clear()\n",
    "        else:\n",
    "            UE_group.append(x[0])\n",
    "            if UE_distance[-1][0] == x[0]:\n",
    "                if x[2] > avg_distance:\n",
    "                    UE_cluster.append(UE_group.copy())\n",
    "                    UE_group.clear()\n",
    "                    UE_group.append(x[1])\n",
    "                    UE_cluster.append(UE_group.copy())\n",
    "                else:\n",
    "                    UE_group.append(x[1])\n",
    "                    UE_cluster.append(UE_group.copy())\n",
    "\n",
    "    print(UE_cluster)\n",
    "    \n",
    "    return UE_cluster\n",
    "    \n",
    "def gap(weights):\n",
    "    if len(weights.shape) == 4:\n",
    "        row = np.mean(weights, axis=1)\n",
    "        result = np.mean(row, axis=0)\n",
    "    elif len(weights.shape) == 2:\n",
    "        result = np.mean(weights, axis=0)\n",
    "    else:\n",
    "        raise print('가중치 잘못됨')\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def split_ue_group(UE_weights, UE_NUM):\n",
    "    layers = UE_weights[0].keys()\n",
    "    concat_weight = {}\n",
    "    concat_mean_weight = {}\n",
    "\n",
    "    for layer in layers:\n",
    "        total = np.zeros(\n",
    "            (UE_NUM, UE_weights[0][layer][0].shape[-2:][0], UE_weights[0][layer][0].shape[-2:][1]))\n",
    "        for i, UE in enumerate(UE_weights):\n",
    "            total[i] = gap(UE[layer][0])\n",
    "        concat_weight[layer] = total\n",
    "\n",
    "    for layer in concat_weight.keys():\n",
    "        concat_mean_weight[layer] = np.mean(concat_weight[layer], axis=0)\n",
    "\n",
    "    UE_high_low = {}\n",
    "    for layer in concat_weight.keys():\n",
    "        true_cnt_list = []\n",
    "        for x in range(UE_NUM):\n",
    "            high_low = concat_weight[layer][x] > concat_mean_weight[layer]\n",
    "            if len(high_low[high_low == True]) >= len(high_low[high_low == False]):\n",
    "                true_cnt_list.append(True)\n",
    "            else:\n",
    "                true_cnt_list.append(False)\n",
    "        UE_high_low[layer] = true_cnt_list\n",
    "        \n",
    "    result = [0 for _ in range(UE_NUM)]\n",
    "    for layer in UE_high_low.keys():\n",
    "        for i, x in enumerate(UE_high_low[layer]):\n",
    "            if x == True:\n",
    "                result[i] += 1\n",
    "\n",
    "    print(result)\n",
    "    \n",
    "    high_ue_list = []\n",
    "    low_ue_list = []\n",
    "\n",
    "    d = dict()\n",
    "    \n",
    "    for i, x in enumerate(result):\n",
    "        d[i] = x\n",
    "        if x >= math.ceil(len(UE_high_low.keys()) / 2):\n",
    "            high_ue_list.append(i)\n",
    "        else:\n",
    "            low_ue_list.append(i)\n",
    "\n",
    "    UE_sort = OrderedDict(sorted(d.items(), key=lambda t:t[1]))\n",
    "    \n",
    "    return gen_ue_cluster(UE_sort)#(high_ue_list, low_ue_list)\n",
    "\n",
    "\n",
    "def gen_UE_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=6, kernel_size=(5, 5),\n",
    "                            strides=1, activation='tanh', input_shape=(32, 32, 1)))\n",
    "    model.add(layers.AveragePooling2D(pool_size=2, strides=2))\n",
    "    model.add(layers.Conv2D(filters=16, kernel_size=(\n",
    "        5, 5), strides=1, activation='tanh'))\n",
    "    model.add(layers.AveragePooling2D(pool_size=2, strides=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(120, activation='tanh'))\n",
    "    model.add(layers.Dense(84, activation='tanh'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='SGD',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # lr = 0.01\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def gen_server_model():\n",
    "    server_model = models.Sequential()\n",
    "    server_model.add(layers.Conv2D(filters=6, kernel_size=(\n",
    "        5, 5), strides=1, activation='tanh', input_shape=(32, 32, 1)))\n",
    "    server_model.add(layers.AveragePooling2D(pool_size=2, strides=2))\n",
    "    server_model.add(layers.Conv2D(\n",
    "        filters=16, kernel_size=(5, 5), strides=1, activation='tanh'))\n",
    "    server_model.add(layers.AveragePooling2D(pool_size=2, strides=2))\n",
    "    server_model.add(layers.Flatten())\n",
    "    server_model.add(layers.Dense(120, activation='tanh'))\n",
    "    server_model.add(layers.Dense(84, activation='tanh'))\n",
    "    server_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return server_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "operating-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)\n",
    "x_test = x_test.reshape(10000, 28, 28, 1)\n",
    "x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
    "x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
    "idx = np.argsort(y_test)\n",
    "x_train_sorted = x_test[idx]\n",
    "y_train_sorted = y_test[idx]\n",
    "\n",
    "UE_NUM = 10\n",
    "\n",
    "UE = []\n",
    "for _ in range(UE_NUM):\n",
    "    UE.append({\"x_train\": [], \"y_train\": []})\n",
    "\n",
    "random.seed(45)\n",
    "\n",
    "total = 0\n",
    "random_num_list = []\n",
    "for _ in range(UE_NUM):\n",
    "    random_num = random.randrange(100, 60000)\n",
    "    total += random_num\n",
    "    random_num_list.append(random_num)\n",
    "\n",
    "x_eval_dataset = x_test.copy()\n",
    "y_eval_dataset = y_test.copy()\n",
    "\n",
    "# start = 0\n",
    "# for i in range(UE_NUM):\n",
    "#     if i == 0:\n",
    "#         UE[i]['x_train'] = x_train[:random_num_list[i]]\n",
    "#         UE[i]['y_train'] = y_train[:random_num_list[i]]\n",
    "#     else:\n",
    "#         UE[i]['x_train'] = x_train[start:start+random_num_list[i]]\n",
    "#         UE[i]['y_train'] = y_train[start:start+random_num_list[i]]\n",
    "#     start += random_num_list[i]\n",
    "\n",
    "for i in range(UE_NUM):\n",
    "    start, end = random.randint(0,20000), random.randint(20001,60000)\n",
    "    UE[i]['x_train'] = x_train[start:end]\n",
    "    UE[i]['y_train'] = y_train[start:end]\n",
    "\n",
    "x_train, x_test, y_train, y_test = [], [], [], []\n",
    "for i in range(UE_NUM):\n",
    "    x_train_temp, x_test_temp, y_train_temp, y_test_temp = train_test_split(\n",
    "        UE[i]['x_train'], UE[i]['y_train'], test_size=0.2, random_state=45)\n",
    "    x_train.append(x_train_temp)\n",
    "    x_test.append(x_test_temp)\n",
    "    y_train.append(y_train_temp)\n",
    "    y_test.append(y_test_temp)\n",
    "\n",
    "df = pd.DataFrame(columns=['High', 'Low', 'High Accuracy', 'High Loss',\n",
    "                           'Low Accuracy', 'Low Loss', 'High Group', 'Low Group'])  # 시뮬레이션 결과를  저장할 데이터프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "standing-evanescence",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 1s 3ms/step - loss: 1.2918 - accuracy: 0.6849 - val_loss: 0.7092 - val_accuracy: 0.8329\n",
      "148/148 [==============================] - 0s 3ms/step - loss: 1.3683 - accuracy: 0.6468 - val_loss: 0.8277 - val_accuracy: 0.8257\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 1.6018 - accuracy: 0.5820 - val_loss: 1.0959 - val_accuracy: 0.7692\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 1.1697 - accuracy: 0.7158 - val_loss: 0.6319 - val_accuracy: 0.8744\n",
      "170/170 [==============================] - 1s 3ms/step - loss: 1.2605 - accuracy: 0.7028 - val_loss: 0.7171 - val_accuracy: 0.8649\n",
      "330/330 [==============================] - 1s 3ms/step - loss: 0.9725 - accuracy: 0.7777 - val_loss: 0.4739 - val_accuracy: 0.8934\n",
      "355/355 [==============================] - 1s 3ms/step - loss: 0.8896 - accuracy: 0.7935 - val_loss: 0.4892 - val_accuracy: 0.8884\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 1.3507 - accuracy: 0.6877 - val_loss: 0.7664 - val_accuracy: 0.8652\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.0754 - accuracy: 0.7485 - val_loss: 0.5590 - val_accuracy: 0.8914\n",
      "208/208 [==============================] - 1s 3ms/step - loss: 1.3151 - accuracy: 0.6784 - val_loss: 0.7078 - val_accuracy: 0.8542\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 2.3005 - accuracy: 0.0837\n",
      "Initial Round -- test loss, test acc: [2.3005309104919434, 0.08370000123977661]\n",
      "[3, 3, 4, 3, 1, 4, 3, 4, 4, 4]\n",
      "[['2'], ['9'], ['0', '3'], ['4'], ['6', '7', '8', '5', '1']]\n",
      "[['2'], ['9'], ['0', '3'], ['4'], ['6', '7', '8', '5', '1']]\n"
     ]
    }
   ],
   "source": [
    "simulation_num = 1\n",
    "for num in range(simulation_num):\n",
    "    # wandb.init(project='Federated Learning',\n",
    "    #            name=f'Simulation {num+1}', entity='yhkim')\n",
    "    save_path = f'simulation_result/{str(num+1)}'\n",
    "    os.mkdir(save_path)\n",
    "    # 연합학습 부분\n",
    "    high_ue_list = []\n",
    "    low_ue_list = []\n",
    "\n",
    "    high_group_global_accuracy = []  # 각 레이어 들의 가중치가 평균보다 높은 UE 그룹의 정확도\n",
    "    high_group_global_loss = []\n",
    "    low_group_global_accuracy = []\n",
    "    low_group_global_loss = []\n",
    "\n",
    "    learning_result_list = []\n",
    "    for i in range(UE_NUM):\n",
    "        model = gen_UE_model()\n",
    "\n",
    "        learning_result_list.append(model.fit(\n",
    "            x_train[i], y_train[i], batch_size=100, epochs=1, validation_data=(x_test[i], y_test[i])))\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    UE_weights = []\n",
    "\n",
    "    for model in learning_result_list:\n",
    "        layer_weights = {}\n",
    "        for x in model.model.layers:\n",
    "            if len(x.get_weights()) > 0:\n",
    "                layer_weights[x.name] = x.get_weights()\n",
    "        UE_weights.append(layer_weights)\n",
    "\n",
    "    server_model = gen_server_model()  # FL 서버 모델 생성\n",
    "\n",
    "    sum_weights = {}\n",
    "\n",
    "    for i in range(len(list(UE_weights[0].keys()))):\n",
    "        weight_shape = [0]\n",
    "        bias_shape = [0]\n",
    "        for dim in UE_weights[0][list(UE_weights[0].keys())[i]][0].shape:\n",
    "            weight_shape.append(dim)\n",
    "        for dim in UE_weights[0][list(UE_weights[0].keys())[i]][1].shape:\n",
    "            bias_shape.append(dim)\n",
    "        sum_weights.update({list(UE_weights[0].keys())[i]: {\n",
    "            'weight': np.empty(weight_shape), 'bias': np.empty(bias_shape)}})\n",
    "\n",
    "        for UE in UE_weights:\n",
    "            sum_weights[list(UE.keys())[i]]['weight'] = np.append(\n",
    "                sum_weights[list(UE.keys())[i]]['weight'], [UE[list(UE.keys())[i]][0]], axis=0)\n",
    "            sum_weights[list(UE.keys())[i]]['bias'] = np.append(\n",
    "                sum_weights[list(UE.keys())[i]]['bias'], [UE[list(UE.keys())[i]][1]], axis=0)\n",
    "\n",
    "    for layer in sum_weights.keys():\n",
    "        for model_layer in server_model.layers:\n",
    "            if layer == model_layer.name:\n",
    "                model_layer.set_weights([np.mean(sum_weights[layer]['weight'], axis=0), np.mean(\n",
    "                    sum_weights[layer]['bias'], axis=0)])\n",
    "\n",
    "    server_model.compile(\n",
    "        optimizer='SGD', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    result = server_model.evaluate(\n",
    "        x=x_eval_dataset, y=y_eval_dataset, batch_size=128)\n",
    "    tf.keras.backend.clear_session()\n",
    "    print('Initial Round -- test loss, test acc:', result)\n",
    "    split_result = split_ue_group(UE_weights, UE_NUM)\n",
    "    print(split_result)\n",
    "    high_ue_list = split_result[0]\n",
    "    low_ue_list = split_result[1]\n",
    "\n",
    "    server_model.save(f'{save_path}/fl_model_gap')\n",
    "\n",
    "    for round in range(100):  # Communication Round, Global epoch\n",
    "        # if os.path.isdir('fl_model_gap'):\n",
    "        high_ue_learning_result_list = []\n",
    "        low_ue_learning_result_list = []\n",
    "\n",
    "        # High Group\n",
    "        for i in high_ue_list:\n",
    "            if round == 0:\n",
    "                model = tf.keras.models.load_model(\n",
    "                    f'{save_path}/fl_model_gap')s\n",
    "                model.compile(optimizer='SGD',\n",
    "                              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                high_ue_learning_result_list.append(model.fit(\n",
    "                    x_train[i], y_train[i], batch_size=100, epochs=1, validation_data=(x_test[i], y_test[i])))\n",
    "                tf.keras.backend.clear_session()\n",
    "            else:\n",
    "                model = tf.keras.models.load_model(\n",
    "                    f'{save_path}/high_group')\n",
    "                model.compile(optimizer='SGD',\n",
    "                              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                high_ue_learning_result_list.append(model.fit(\n",
    "                    x_train[i], y_train[i], batch_size=100, epochs=1, validation_data=(x_test[i], y_test[i])))\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "        UE_weights = []\n",
    "\n",
    "        for model in high_ue_learning_result_list:\n",
    "            layer_weights = {}\n",
    "            for x in model.model.layers:\n",
    "                if len(x.get_weights()) > 0:\n",
    "                    layer_weights[x.name] = x.get_weights()\n",
    "            UE_weights.append(layer_weights)\n",
    "\n",
    "        server_model = gen_server_model()\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        sum_weights = {}\n",
    "\n",
    "        for i in range(len(list(UE_weights[0].keys()))):\n",
    "            weight_shape = [0]\n",
    "            bias_shape = [0]\n",
    "            for dim in UE_weights[0][list(UE_weights[0].keys())[i]][0].shape:\n",
    "                weight_shape.append(dim)\n",
    "            for dim in UE_weights[0][list(UE_weights[0].keys())[i]][1].shape:\n",
    "                bias_shape.append(dim)\n",
    "            sum_weights.update({list(UE_weights[0].keys())[i]: {\n",
    "                'weight': np.empty(weight_shape), 'bias': np.empty(bias_shape)}})\n",
    "\n",
    "            for UE in UE_weights:\n",
    "                sum_weights[list(UE.keys())[i]]['weight'] = np.append(\n",
    "                    sum_weights[list(UE.keys())[i]]['weight'], [UE[list(UE.keys())[i]][0]], axis=0)\n",
    "                sum_weights[list(UE.keys())[i]]['bias'] = np.append(\n",
    "                    sum_weights[list(UE.keys())[i]]['bias'], [UE[list(UE.keys())[i]][1]], axis=0)\n",
    "\n",
    "        for layer in sum_weights.keys():\n",
    "            for model_layer in server_model.layers:\n",
    "                if layer == model_layer.name:\n",
    "                    model_layer.set_weights([np.mean(sum_weights[layer]['weight'], axis=0), np.mean(\n",
    "                        sum_weights[layer]['bias'], axis=0)])\n",
    "\n",
    "        server_model.compile(\n",
    "            optimizer='SGD', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        result = server_model.evaluate(\n",
    "            x=x_eval_dataset, y=y_eval_dataset, batch_size=128)\n",
    "        print(f'Round {round} -- test loss, test acc: {result}')\n",
    "\n",
    "        server_model.save(f'{save_path}/high_group')\n",
    "        high_group_global_loss.append(result[0])\n",
    "        high_group_global_accuracy.append(result[1])\n",
    "\n",
    "        # Low Group\n",
    "\n",
    "        for i in low_ue_list:\n",
    "            if round == 0:\n",
    "                model = tf.keras.models.load_model(\n",
    "                    f'{save_path}/fl_model_gap')\n",
    "                model.compile(optimizer='SGD',\n",
    "                              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                low_ue_learning_result_list.append(model.fit(\n",
    "                    x_train[i], y_train[i], batch_size=100, epochs=1, validation_data=(x_test[i], y_test[i])))\n",
    "                tf.keras.backend.clear_session()\n",
    "            else:\n",
    "                model = tf.keras.models.load_model(\n",
    "                    f'{save_path}/low_group')\n",
    "                model.compile(optimizer='SGD',\n",
    "                              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                low_ue_learning_result_list.append(model.fit(\n",
    "                    x_train[i], y_train[i], batch_size=100, epochs=1, validation_data=(x_test[i], y_test[i])))\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "        UE_weights = []\n",
    "\n",
    "        for model in low_ue_learning_result_list:\n",
    "            layer_weights = {}\n",
    "            for x in model.model.layers:\n",
    "                if len(x.get_weights()) > 0:\n",
    "                    layer_weights[x.name] = x.get_weights()\n",
    "            UE_weights.append(layer_weights)\n",
    "\n",
    "        server_model = gen_server_model()\n",
    "\n",
    "        sum_weights = {}\n",
    "\n",
    "        for i in range(len(list(UE_weights[0].keys()))):\n",
    "            weight_shape = [0]\n",
    "            bias_shape = [0]\n",
    "            for dim in UE_weights[0][list(UE_weights[0].keys())[i]][0].shape:\n",
    "                weight_shape.append(dim)\n",
    "            for dim in UE_weights[0][list(UE_weights[0].keys())[i]][1].shape:\n",
    "                bias_shape.append(dim)\n",
    "            sum_weights.update({list(UE_weights[0].keys())[i]: {\n",
    "                'weight': np.empty(weight_shape), 'bias': np.empty(bias_shape)}})\n",
    "\n",
    "            for UE in UE_weights:\n",
    "                sum_weights[list(UE.keys())[i]]['weight'] = np.append(\n",
    "                    sum_weights[list(UE.keys())[i]]['weight'], [UE[list(UE.keys())[i]][0]], axis=0)\n",
    "                sum_weights[list(UE.keys())[i]]['bias'] = np.append(\n",
    "                    sum_weights[list(UE.keys())[i]]['bias'], [UE[list(UE.keys())[i]][1]], axis=0)\n",
    "\n",
    "        for layer in sum_weights.keys():\n",
    "            for model_layer in server_model.layers:\n",
    "                if layer == model_layer.name:\n",
    "                    model_layer.set_weights([np.mean(sum_weights[layer]['weight'], axis=0), np.mean(\n",
    "                        sum_weights[layer]['bias'], axis=0)])\n",
    "\n",
    "        server_model.compile(\n",
    "            optimizer='SGD', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        result = server_model.evaluate(\n",
    "            x=x_eval_dataset, y=y_eval_dataset, batch_size=128)\n",
    "        tf.keras.backend.clear_session()\n",
    "        print(f'Round {round} -- test loss, test acc: {result}')\n",
    "\n",
    "        server_model.save(f'{save_path}/low_group')\n",
    "        low_group_global_loss.append(result[0])\n",
    "        low_group_global_accuracy.append(result[1])\n",
    "    #     wandb.log(\n",
    "    #         {'high group global accuracy': high_group_global_accuracy[round], 'high group global loss': high_group_global_loss[round], 'low group global accuracy': low_group_global_accuracy[round], 'low group global loss': low_group_global_loss[round], 'global epoch': round+1,\n",
    "    #          'high_ue_list': len(high_ue_list), 'low_ue_list': len(low_ue_list), 'high_ue_group': str(high_ue_list)[1:-1], 'low_ue_group': str(low_ue_list)[1:-1]})\n",
    "    # wandb.finish()\n",
    "    df.loc[num] = [len(high_ue_list), len(low_ue_list), high_group_global_accuracy[-1],\n",
    "                   high_group_global_loss[-1], low_group_global_accuracy[-1], low_group_global_loss[-1], str(high_ue_list)[1:-1], str(low_ue_list)[1:-1]]\n",
    "    df.to_excel('simulation_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-capacity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
